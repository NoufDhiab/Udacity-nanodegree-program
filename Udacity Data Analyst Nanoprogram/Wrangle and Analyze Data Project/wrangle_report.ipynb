{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "Prepared by: Nouf AlGhamdi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content:\n",
    "1. Project Overview\n",
    "2. Gathering Data \n",
    "3. Assessing Data\n",
    "4. Cleaning Data\n",
    "5. Storing Data    \n",
    "6. Visualizing Data\n",
    "7. Project Challenges and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling data, also known as data cleansing, data remediation, or data munging, involves transforming raw data into easily usable formats.\n",
    "Methods vary depending on the data being utilized and the goal being pursued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we have wrangled (and analyzed and visualized) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs along with a humorous commentary about the dog. The WeRateDogs account has over 4 million followers and has been covered by international media outlets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses three different datasets, which were obtained in the following manner: \n",
    "#### 1. The WeRateDogs Twitter archive:\n",
    "To use this dataset we Downloaded the file twitter_archive_enhanced.csv manually by clicking on the provided link Once it is downloaded,  we upload it and read the data into a pandas DataFrame __twitter_archive__ .\n",
    "#### 2. The tweet image predictions\n",
    "This file (image_predictions.tsv) is present in each tweet according to a neural network. It is hosted on Udacity's servers so we downloaded programmatically using the Requests library and the provided URL, and stored it into __image_predictions__ DataFrame\n",
    "#### 3. Additional data from the Twitter API\n",
    "We also gathered each tweet's retweet count and favorite (\"like\") count and additional data we find interesting like the folower count, and the friends count. Using the tweet IDs in the WeRateDogs Twitter archive, and have query the Twitter API for each tweet's JSON data using Python's Tweepy library and storeed each tweet's entire set of JSON data in a file called tweet_json.txt file. We store this additional data into the DataFrame __tweet_detailed_data__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assessing Data\n",
    "After gathering all the three pieces of data,  we assessed them visually and programmatically for quality and tidiness issues. we have Detected and documented __ten (10) quality issues__ and __four (4) tidiness issues__, The issues identified during the assessment phase were documented into the __Assessing Data__ section of a Jupyter notebook and categorized into Quality Issues and Tidiness Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Cleaning Data\n",
    "In this section, we cleaned all of the issues we documented while assessing.\n",
    "also as an anssioal step we have a copy of the original data before the cleaning.\n",
    "And best approach to clean data in this three steps:\n",
    "* Define — express in words how you intend to resolve the problem.\n",
    "* Code — turn your definitions into executable code.\n",
    "* Test — test your data to confirm that your code was properly implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data had been cleaned, individual pieces were merged according to the rules of tidy data into a high-quality and tidy DataFrame __merged_dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we have stored the cleaned DataFrame in a CSV file named __twitter_archive_master.csv__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cleaned dataset was used to derive insights and make visualizations. These latter will be presented in the act report file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Project Challenges and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found working with Twitter API to be the most challenging aspect of this project. Even though I completed all the steps required to make the project work, I was not granted access to some of the main functions that were necessary to gather the information needed for the project, however the __tweet_json.txt__ file was provided to make this task easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
